{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHiT6FHn+cRiFFP91HzKeQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manuaishika/softkmeans-nn/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29-C_g6BojwN",
        "outputId": "573a67f3-58c3-4958-d95a-efbb3199d7c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "GPU available: False\n",
            "üöÄ Soft K-Means Neural Network - Google Colab Ready!\n",
            "\n",
            "Run one of these functions:\n",
            "1. quick_demo() - For a quick test\n",
            "2. run_experiment('moons') - For moon dataset\n",
            "3. run_all_experiments() - For all datasets\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# SOFT K-MEANS NEURAL NETWORK - GOOGLE COLAB\n",
        "# ============================================\n",
        "\n",
        "# Step 1: Install/Import everything\n",
        "!pip install torch torchvision scikit-learn matplotlib -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# ============================================\n",
        "# 1. SOFT K-MEANS LAYER\n",
        "# ============================================\n",
        "\n",
        "class SoftKMeansLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Soft K-Means layer that learns cluster centroids\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, num_clusters, temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.num_clusters = num_clusters\n",
        "        self.temperature = temperature\n",
        "        self.temperature_factor = nn.Parameter(torch.tensor([temperature]))\n",
        "\n",
        "        # Initialize centroids with Xavier initialization\n",
        "        self.centroids = nn.Parameter(torch.randn(num_clusters, input_dim) * 0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, input_dim)\n",
        "        # centroids shape: (num_clusters, input_dim)\n",
        "\n",
        "        # Compute squared Euclidean distance\n",
        "        x_norm = (x ** 2).sum(dim=1, keepdim=True)\n",
        "        c_norm = (self.centroids ** 2).sum(dim=1, keepdim=True).t()\n",
        "\n",
        "        distances = x_norm + c_norm - 2 * torch.mm(x, self.centroids.t())\n",
        "\n",
        "        # Apply temperature and get soft assignments\n",
        "        logits = -distances / self.temperature_factor\n",
        "        responsibilities = F.softmax(logits, dim=1)\n",
        "\n",
        "        return responsibilities, distances\n",
        "\n",
        "    def get_centroids(self):\n",
        "        return self.centroids.data\n",
        "\n",
        "# ============================================\n",
        "# 2. COMPLETE NEURAL NETWORK MODEL\n",
        "# ============================================\n",
        "\n",
        "class SoftKMeansNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural Network with feature extraction + Soft K-Means\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dims, num_clusters, temperature=1.0):\n",
        "        super().__init__()\n",
        "\n",
        "        # Build feature extraction layers\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        for i, hidden_dim in enumerate(hidden_dims):\n",
        "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
        "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(0.2))\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        self.encoder = nn.Sequential(*layers)\n",
        "\n",
        "        # Soft K-Means clustering layer\n",
        "        self.soft_kmeans = SoftKMeansLayer(prev_dim, num_clusters, temperature)\n",
        "\n",
        "        # Optional decoder for reconstruction (if needed)\n",
        "        self.decoder_layers = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encode to features\n",
        "        features = self.encoder(x)\n",
        "\n",
        "        # Get soft assignments\n",
        "        responsibilities, distances = self.soft_kmeans(features)\n",
        "\n",
        "        return features, responsibilities, distances\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"Get hard cluster assignments\"\"\"\n",
        "        with torch.no_grad():\n",
        "            _, responsibilities, _ = self.forward(x)\n",
        "            return torch.argmax(responsibilities, dim=1)\n",
        "\n",
        "    def get_centroids(self):\n",
        "        return self.soft_kmeans.get_centroids()\n",
        "\n",
        "    def get_soft_assignments(self, x):\n",
        "        \"\"\"Get soft assignment probabilities\"\"\"\n",
        "        with torch.no_grad():\n",
        "            _, responsibilities, _ = self.forward(x)\n",
        "            return responsibilities\n",
        "\n",
        "# ============================================\n",
        "# 3. TRAINER WITH MULTIPLE LOSS OPTIONS\n",
        "# ============================================\n",
        "\n",
        "class SoftKMeansTrainer:\n",
        "    def __init__(self, model, learning_rate=0.001, lambda_reg=0.01):\n",
        "        self.model = model\n",
        "        self.model.to(device)\n",
        "        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "        self.lambda_reg = lambda_reg\n",
        "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer, mode='min', patience=10, factor=0.5, verbose=True\n",
        "        )\n",
        "\n",
        "    def kmeans_loss(self, responsibilities, distances):\n",
        "        \"\"\"Standard K-Means loss\"\"\"\n",
        "        return torch.sum(responsibilities * distances)\n",
        "\n",
        "    def entropy_regularization(self, responsibilities):\n",
        "        \"\"\"Encourage confident assignments\"\"\"\n",
        "        entropy = -torch.sum(responsibilities * torch.log(responsibilities + 1e-10), dim=1)\n",
        "        return torch.mean(entropy)\n",
        "\n",
        "    def centroid_regularization(self):\n",
        "        \"\"\"Prevent centroids from collapsing\"\"\"\n",
        "        centroids = self.model.get_centroids()\n",
        "        centroid_distances = torch.cdist(centroids, centroids, p=2)\n",
        "        mask = ~torch.eye(centroid_distances.size(0), dtype=torch.bool).to(device)\n",
        "        min_distance = torch.min(centroid_distances[mask])\n",
        "        return 1.0 / (min_distance + 1e-10)\n",
        "\n",
        "    def train_step(self, x_batch):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        _, responsibilities, distances = self.model(x_batch)\n",
        "\n",
        "        # Compute losses\n",
        "        main_loss = self.kmeans_loss(responsibilities, distances)\n",
        "        entropy_loss = self.entropy_regularization(responsibilities)\n",
        "        reg_loss = self.centroid_regularization()\n",
        "\n",
        "        # Total loss\n",
        "        total_loss = main_loss + 0.1 * entropy_loss + self.lambda_reg * reg_loss\n",
        "\n",
        "        # Backward pass\n",
        "        total_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return {\n",
        "            'total_loss': total_loss.item(),\n",
        "            'kmeans_loss': main_loss.item(),\n",
        "            'entropy_loss': entropy_loss.item(),\n",
        "            'reg_loss': reg_loss.item()\n",
        "        }\n",
        "\n",
        "    def train(self, data_loader, num_epochs=100, verbose=True):\n",
        "        history = {\n",
        "            'total_loss': [], 'kmeans_loss': [],\n",
        "            'entropy_loss': [], 'reg_loss': []\n",
        "        }\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            epoch_losses = {'total_loss': 0, 'kmeans_loss': 0,\n",
        "                          'entropy_loss': 0, 'reg_loss': 0}\n",
        "\n",
        "            for x_batch, _ in data_loader:\n",
        "                x_batch = x_batch.to(device)\n",
        "                losses = self.train_step(x_batch)\n",
        "\n",
        "                for key in losses:\n",
        "                    epoch_losses[key] += losses[key]\n",
        "\n",
        "            # Average losses\n",
        "            for key in epoch_losses:\n",
        "                epoch_losses[key] /= len(data_loader)\n",
        "                history[key].append(epoch_losses[key])\n",
        "\n",
        "            # Update learning rate\n",
        "            self.scheduler.step(epoch_losses['total_loss'])\n",
        "\n",
        "            if verbose and (epoch + 1) % 10 == 0:\n",
        "                print(f\"Epoch [{epoch+1:3d}/{num_epochs}]: \"\n",
        "                      f\"Total Loss: {epoch_losses['total_loss']:.4f}, \"\n",
        "                      f\"K-Means: {epoch_losses['kmeans_loss']:.4f}, \"\n",
        "                      f\"Entropy: {epoch_losses['entropy_loss']:.4f}\")\n",
        "\n",
        "        return history\n",
        "\n",
        "# ============================================\n",
        "# 4. DATA GENERATION AND VISUALIZATION\n",
        "# ============================================\n",
        "\n",
        "def create_dataset(dataset_type='blobs', n_samples=1000, n_features=2, n_clusters=4):\n",
        "    \"\"\"Create different types of datasets\"\"\"\n",
        "\n",
        "    if dataset_type == 'blobs':\n",
        "        X, y = make_blobs(\n",
        "            n_samples=n_samples,\n",
        "            n_features=n_features,\n",
        "            centers=n_clusters,\n",
        "            cluster_std=0.8,\n",
        "            random_state=42\n",
        "        )\n",
        "    elif dataset_type == 'moons':\n",
        "        X, y = make_moons(n_samples=n_samples, noise=0.1, random_state=42)\n",
        "        n_clusters = 2\n",
        "    elif dataset_type == 'circles':\n",
        "        X, y = make_circles(n_samples=n_samples, factor=0.5, noise=0.05, random_state=42)\n",
        "        n_clusters = 2\n",
        "    elif dataset_type == 'aniso':\n",
        "        X, y = make_blobs(n_samples=n_samples, centers=n_clusters, random_state=170)\n",
        "        transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
        "        X = np.dot(X, transformation)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset type: {dataset_type}\")\n",
        "\n",
        "    # Normalize data\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "\n",
        "    # Convert to tensors\n",
        "    X_tensor = torch.FloatTensor(X).to(device)\n",
        "    y_tensor = torch.LongTensor(y).to(device)\n",
        "\n",
        "    # Create data loader\n",
        "    dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=64, shuffle=True\n",
        "    )\n",
        "\n",
        "    return X_tensor, y_tensor, data_loader, n_clusters\n",
        "\n",
        "def visualize_results(X, predictions, soft_probs, centroids,\n",
        "                      history=None, dataset_name=\"Dataset\"):\n",
        "    \"\"\"Visualize clustering results with multiple plots\"\"\"\n",
        "\n",
        "    fig = plt.figure(figsize=(18, 10))\n",
        "\n",
        "    # Plot 1: Clustering results\n",
        "    ax1 = plt.subplot(2, 3, 1)\n",
        "    scatter = ax1.scatter(X[:, 0], X[:, 1], c=predictions,\n",
        "                         cmap='tab20', alpha=0.6, s=30)\n",
        "    if centroids is not None:\n",
        "        ax1.scatter(centroids[:, 0], centroids[:, 1],\n",
        "                   c='red', marker='X', s=300, linewidths=2,\n",
        "                   edgecolor='black', label='Centroids')\n",
        "    ax1.set_title(f'{dataset_name} - Cluster Assignments')\n",
        "    ax1.set_xlabel('Feature 1')\n",
        "    ax1.set_ylabel('Feature 2')\n",
        "    ax1.legend()\n",
        "    plt.colorbar(scatter, ax=ax1)\n",
        "\n",
        "    # Plot 2: Soft assignment probabilities\n",
        "    ax2 = plt.subplot(2, 3, 2)\n",
        "    uncertainty = 1 - np.max(soft_probs, axis=1)\n",
        "    sc = ax2.scatter(X[:, 0], X[:, 1], c=uncertainty,\n",
        "                    cmap='viridis', alpha=0.6, s=30)\n",
        "    ax2.set_title('Assignment Uncertainty')\n",
        "    ax2.set_xlabel('Feature 1')\n",
        "    ax2.set_ylabel('Feature 2')\n",
        "    plt.colorbar(sc, ax=ax2)\n",
        "\n",
        "    # Plot 3: Top 2 probabilities\n",
        "    ax3 = plt.subplot(2, 3, 3)\n",
        "    top2_diff = np.sort(soft_probs, axis=1)[:, -1] - np.sort(soft_probs, axis=1)[:, -2]\n",
        "    sc = ax3.scatter(X[:, 0], X[:, 1], c=top2_diff,\n",
        "                    cmap='coolwarm', alpha=0.6, s=30)\n",
        "    ax3.set_title('Difference: Top 2 Probabilities')\n",
        "    ax3.set_xlabel('Feature 1')\n",
        "    ax3.set_ylabel('Feature 2')\n",
        "    plt.colorbar(sc, ax=ax3)\n",
        "\n",
        "    # Plot 4: Loss curves\n",
        "    if history is not None:\n",
        "        ax4 = plt.subplot(2, 3, 4)\n",
        "        ax4.plot(history['total_loss'], label='Total Loss', linewidth=2)\n",
        "        ax4.plot(history['kmeans_loss'], label='K-Means Loss', linewidth=2)\n",
        "        ax4.set_title('Training Loss Curves')\n",
        "        ax4.set_xlabel('Epoch')\n",
        "        ax4.set_ylabel('Loss')\n",
        "        ax4.legend()\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "\n",
        "        ax5 = plt.subplot(2, 3, 5)\n",
        "        ax5.plot(history['entropy_loss'], label='Entropy Loss', color='green', linewidth=2)\n",
        "        ax5.plot(history['reg_loss'], label='Reg Loss', color='red', linewidth=2)\n",
        "        ax5.set_title('Auxiliary Losses')\n",
        "        ax5.set_xlabel('Epoch')\n",
        "        ax5.set_ylabel('Loss')\n",
        "        ax5.legend()\n",
        "        ax5.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 6: Probability distribution\n",
        "    ax6 = plt.subplot(2, 3, 6)\n",
        "    for cluster in range(soft_probs.shape[1]):\n",
        "        ax6.hist(soft_probs[:, cluster], bins=30, alpha=0.5,\n",
        "                label=f'Cluster {cluster}')\n",
        "    ax6.set_title('Probability Distribution per Cluster')\n",
        "    ax6.set_xlabel('Probability')\n",
        "    ax6.set_ylabel('Frequency')\n",
        "    ax6.legend()\n",
        "    ax6.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print metrics\n",
        "    if predictions is not None:\n",
        "        print(f\"Number of clusters: {len(np.unique(predictions))}\")\n",
        "        print(f\"Cluster sizes: {np.bincount(predictions)}\")\n",
        "        silhouette = silhouette_score(X, predictions)\n",
        "        print(f\"Silhouette Score: {silhouette:.4f}\")\n",
        "\n",
        "# ============================================\n",
        "# 5. MAIN EXPERIMENT FUNCTION\n",
        "# ============================================\n",
        "\n",
        "def run_experiment(dataset_type='blobs', n_samples=1000,\n",
        "                   hidden_dims=[32, 16], num_epochs=100):\n",
        "    \"\"\"Run complete experiment from data to visualization\"\"\"\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"EXPERIMENT: {dataset_type.upper()} Dataset\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Create dataset\n",
        "    X, y_true, data_loader, n_clusters = create_dataset(\n",
        "        dataset_type=dataset_type,\n",
        "        n_samples=n_samples,\n",
        "        n_clusters=4 if dataset_type == 'blobs' else 2\n",
        "    )\n",
        "\n",
        "    # Initialize model\n",
        "    model = SoftKMeansNN(\n",
        "        input_dim=X.shape[1],\n",
        "        hidden_dims=hidden_dims,\n",
        "        num_clusters=n_clusters,\n",
        "        temperature=0.5\n",
        "    )\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = SoftKMeansTrainer(\n",
        "        model,\n",
        "        learning_rate=0.001,\n",
        "        lambda_reg=0.01\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    print(\"\\nTraining Soft K-Means Neural Network...\")\n",
        "    history = trainer.train(\n",
        "        data_loader,\n",
        "        num_epochs=num_epochs,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        predictions = model.predict(X).cpu().numpy()\n",
        "        soft_probs = model.get_soft_assignments(X).cpu().numpy()\n",
        "        centroids = model.get_centroids().cpu().numpy()\n",
        "\n",
        "    # Visualize results\n",
        "    print(\"\\nVisualizing results...\")\n",
        "    visualize_results(\n",
        "        X.cpu().numpy(),\n",
        "        predictions,\n",
        "        soft_probs,\n",
        "        centroids,\n",
        "        history,\n",
        "        dataset_name=dataset_type.capitalize()\n",
        "    )\n",
        "\n",
        "    return model, predictions, history\n",
        "\n",
        "# ============================================\n",
        "# 6. RUN MULTIPLE EXPERIMENTS\n",
        "# ============================================\n",
        "\n",
        "def run_all_experiments():\n",
        "    \"\"\"Run experiments on different dataset types\"\"\"\n",
        "\n",
        "    dataset_types = ['blobs', 'moons', 'circles']\n",
        "    results = {}\n",
        "\n",
        "    for dataset_type in dataset_types:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(f\"Running experiment on {dataset_type} dataset...\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Adjust hidden dimensions based on dataset complexity\n",
        "        if dataset_type == 'blobs':\n",
        "            hidden_dims = [16, 8]  # Simpler network for blobs\n",
        "        else:\n",
        "            hidden_dims = [32, 16, 8]  # Deeper network for complex shapes\n",
        "\n",
        "        model, predictions, history = run_experiment(\n",
        "            dataset_type=dataset_type,\n",
        "            n_samples=1000,\n",
        "            hidden_dims=hidden_dims,\n",
        "            num_epochs=80\n",
        "        )\n",
        "\n",
        "        results[dataset_type] = {\n",
        "            'model': model,\n",
        "            'predictions': predictions,\n",
        "            'history': history\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "# ============================================\n",
        "# 7. QUICK DEMO (Run this first!)\n",
        "# ============================================\n",
        "\n",
        "def quick_demo():\n",
        "    \"\"\"Quick demonstration of soft k-means\"\"\"\n",
        "    print(\"üîç QUICK DEMO: Soft K-Means Neural Network\")\n",
        "\n",
        "    # Simple example\n",
        "    X_tensor, y_true, data_loader, n_clusters = create_dataset(\n",
        "        dataset_type='blobs',\n",
        "        n_samples=500,\n",
        "        n_clusters=3\n",
        "    )\n",
        "\n",
        "    # Simple model\n",
        "    model = SoftKMeansNN(\n",
        "        input_dim=2,\n",
        "        hidden_dims=[10],\n",
        "        num_clusters=n_clusters,\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    # Quick training\n",
        "    trainer = SoftKMeansTrainer(model, learning_rate=0.01)\n",
        "\n",
        "    print(\"\\nTraining for 30 epochs...\")\n",
        "    history = trainer.train(data_loader, num_epochs=30, verbose=True)\n",
        "\n",
        "    # Get results\n",
        "    predictions = model.predict(X_tensor).cpu().numpy()\n",
        "    soft_probs = model.get_soft_assignments(X_tensor).cpu().numpy()\n",
        "    centroids = model.get_centroids().cpu().numpy()\n",
        "\n",
        "    # Quick visualization\n",
        "    plt.figure(figsize=(15, 4))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.scatter(X_tensor.cpu()[:, 0], X_tensor.cpu()[:, 1],\n",
        "                c=predictions, cmap='viridis', alpha=0.6)\n",
        "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
        "                c='red', marker='X', s=200, label='Centroids')\n",
        "    plt.title(\"Cluster Assignments\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.scatter(X_tensor.cpu()[:, 0], X_tensor.cpu()[:, 1],\n",
        "                c=np.max(soft_probs, axis=1), cmap='plasma', alpha=0.6)\n",
        "    plt.title(\"Maximum Probability\")\n",
        "    plt.colorbar()\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(history['total_loss'], label='Total Loss')\n",
        "    plt.plot(history['kmeans_loss'], label='K-Means Loss')\n",
        "    plt.title(\"Training Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n‚úÖ Demo completed!\")\n",
        "    print(f\"Final centroids:\\n{centroids}\")\n",
        "    print(f\"\\nCluster distribution: {np.bincount(predictions)}\")\n",
        "\n",
        "# ============================================\n",
        "# 8. HOW TO RUN\n",
        "# ============================================\n",
        "\n",
        "\"\"\"\n",
        "INSTRUCTIONS FOR GOOGLE COLAB:\n",
        "\n",
        "1. Open Google Colab: https://colab.research.google.com\n",
        "2. Create a new notebook\n",
        "3. Copy ALL this code into a cell\n",
        "4. Run ONE of these commands:\n",
        "\n",
        "Option A: Quick Demo (Recommended first)\n",
        "\"\"\"\n",
        "# quick_demo()\n",
        "\n",
        "\"\"\"\n",
        "Option B: Single Experiment\n",
        "\"\"\"\n",
        "# run_experiment(dataset_type='moons', num_epochs=50)\n",
        "\n",
        "\"\"\"\n",
        "Option C: All Experiments\n",
        "\"\"\"\n",
        "# results = run_all_experiments()\n",
        "\n",
        "\"\"\"\n",
        "Option D: Custom Experiment\n",
        "\"\"\"\n",
        "# # Create custom data\n",
        "# X, y = make_blobs(n_samples=1000, centers=5, random_state=42)\n",
        "# X = StandardScaler().fit_transform(X)\n",
        "# X_tensor = torch.FloatTensor(X).to(device)\n",
        "\n",
        "# # Create model\n",
        "# model = SoftKMeansNN(\n",
        "#     input_dim=2,\n",
        "#     hidden_dims=[32, 16],\n",
        "#     num_clusters=5,\n",
        "#     temperature=0.4\n",
        "# )\n",
        "\n",
        "# # Train\n",
        "# trainer = SoftKMeansTrainer(model)\n",
        "# dataset = torch.utils.data.TensorDataset(X_tensor, torch.zeros(len(X_tensor)))\n",
        "# data_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "# history = trainer.train(data_loader, num_epochs=100)\n",
        "\n",
        "# ============================================\n",
        "# RUN THE QUICK DEMO FIRST!\n",
        "# ============================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üöÄ Soft K-Means Neural Network - Google Colab Ready!\")\n",
        "    print(\"\\nRun one of these functions:\")\n",
        "    print(\"1. quick_demo() - For a quick test\")\n",
        "    print(\"2. run_experiment('moons') - For moon dataset\")\n",
        "    print(\"3. run_all_experiments() - For all datasets\")\n",
        "\n",
        "    # Uncomment one line below to run:\n",
        "    # quick_demo()\n",
        "    # run_experiment('blobs')\n",
        "    # results = run_all_experiments()"
      ]
    }
  ]
}